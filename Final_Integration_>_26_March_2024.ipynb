{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiDS+8QGX/pq32ZYfApbXZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harikaraja/News_headline_Generation-using-NLP/blob/Final-Integration/Final_Integration_%3E_26_March_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM8o_OPheqv6",
        "outputId": "7deb9ab8-1de6-44f1-db19-12c9548157f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp.stanford.edu/software/stanford-corenlp-4.5.6.zip\n",
        "\n",
        "!unzip stanford-corenlp-4.5.6.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lCeLzjJeyru",
        "outputId": "a3731200-af0a-4bc1-9be5-5637c251eee5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-26 12:10:28--  https://nlp.stanford.edu/software/stanford-corenlp-4.5.6.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-4.5.6.zip [following]\n",
            "--2024-03-26 12:10:28--  https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-4.5.6.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 505656981 (482M) [application/zip]\n",
            "Saving to: ‘stanford-corenlp-4.5.6.zip’\n",
            "\n",
            "stanford-corenlp-4. 100%[===================>] 482.23M  5.00MB/s    in 91s     \n",
            "\n",
            "2024-03-26 12:11:59 (5.29 MB/s) - ‘stanford-corenlp-4.5.6.zip’ saved [505656981/505656981]\n",
            "\n",
            "Archive:  stanford-corenlp-4.5.6.zip\n",
            "   creating: stanford-corenlp-4.5.6/\n",
            "  inflating: stanford-corenlp-4.5.6/CoreNLP-to-HTML.xsl  \n",
            "  inflating: stanford-corenlp-4.5.6/input.txt  \n",
            "  inflating: stanford-corenlp-4.5.6/stanford-corenlp-4.5.6-javadoc.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/stanford-corenlp-4.5.6-models.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/input.txt.out  \n",
            "  inflating: stanford-corenlp-4.5.6/pom-java-11.xml  \n",
            "  inflating: stanford-corenlp-4.5.6/LIBRARY-LICENSES  \n",
            "  inflating: stanford-corenlp-4.5.6/javax.activation-api-1.2.0.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/jaxb-impl-2.4.0-b180830.0438.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/build.xml  \n",
            "   creating: stanford-corenlp-4.5.6/patterns/\n",
            "  inflating: stanford-corenlp-4.5.6/patterns/example.properties  \n",
            "  inflating: stanford-corenlp-4.5.6/patterns/names.txt  \n",
            "  inflating: stanford-corenlp-4.5.6/patterns/goldnames.txt  \n",
            "  inflating: stanford-corenlp-4.5.6/patterns/presidents.txt  \n",
            " extracting: stanford-corenlp-4.5.6/patterns/places.txt  \n",
            "  inflating: stanford-corenlp-4.5.6/patterns/stopwords.txt  \n",
            " extracting: stanford-corenlp-4.5.6/patterns/otherpeople.txt  \n",
            " extracting: stanford-corenlp-4.5.6/patterns/goldplaces.txt  \n",
            "  inflating: stanford-corenlp-4.5.6/RESOURCE-LICENSES  \n",
            "  inflating: stanford-corenlp-4.5.6/pom.xml  \n",
            "  inflating: stanford-corenlp-4.5.6/StanfordDependenciesManual.pdf  \n",
            "   creating: stanford-corenlp-4.5.6/tokensregex/\n",
            "  inflating: stanford-corenlp-4.5.6/tokensregex/color.rules.txt  \n",
            "  inflating: stanford-corenlp-4.5.6/tokensregex/color.properties  \n",
            "  inflating: stanford-corenlp-4.5.6/tokensregex/retokenize.txt  \n",
            "  inflating: stanford-corenlp-4.5.6/tokensregex/color.input.txt  \n",
            "  inflating: stanford-corenlp-4.5.6/ejml-core-0.39.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/StanfordCoreNlpDemo.java  \n",
            "  inflating: stanford-corenlp-4.5.6/pom-java-17.xml  \n",
            "  inflating: stanford-corenlp-4.5.6/slf4j-simple.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/javax.json.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/ejml-ddense-0.39-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/ejml-simple-0.39-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/joda-time-2.10.5-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/jollyday-0.4.9-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/LICENSE.txt  \n",
            "  inflating: stanford-corenlp-4.5.6/istack-commons-runtime-3.0.7-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/Makefile  \n",
            "  inflating: stanford-corenlp-4.5.6/corenlp.sh  \n",
            "  inflating: stanford-corenlp-4.5.6/xom.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/stanford-corenlp-4.5.6.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/javax.activation-api-1.2.0-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/jaxb-api-2.4.0-b180830.0359.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/ejml-simple-0.39.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/ejml-core-0.39-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/joda-time.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/javax.json-api-1.0-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/input.txt.xml  \n",
            "  inflating: stanford-corenlp-4.5.6/xom-1.3.9-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/stanford-corenlp-4.5.6-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/ShiftReduceDemo.java  \n",
            "  inflating: stanford-corenlp-4.5.6/jaxb-api-2.4.0-b180830.0359-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/ejml-ddense-0.39.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/jaxb-impl-2.4.0-b180830.0438-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/SemgrexDemo.java  \n",
            "  inflating: stanford-corenlp-4.5.6/jollyday.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/slf4j-api.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/README.txt  \n",
            "  inflating: stanford-corenlp-4.5.6/sample-project-pom.xml  \n",
            "   creating: stanford-corenlp-4.5.6/sutime/\n",
            "  inflating: stanford-corenlp-4.5.6/sutime/defs.sutime.txt  \n",
            "  inflating: stanford-corenlp-4.5.6/sutime/british.sutime.txt  \n",
            "  inflating: stanford-corenlp-4.5.6/sutime/english.holidays.sutime.txt  \n",
            "  inflating: stanford-corenlp-4.5.6/sutime/english.sutime.txt  \n",
            "  inflating: stanford-corenlp-4.5.6/sutime/spanish.sutime.txt  \n",
            "  inflating: stanford-corenlp-4.5.6/istack-commons-runtime-3.0.7.jar  \n",
            "  inflating: stanford-corenlp-4.5.6/protobuf-java-3.19.6.jar  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jamYMN7EfV4E",
        "outputId": "5e92391d-84d5-4378-ead1-bb4f94ff5255"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textacy\n",
            "  Downloading textacy-0.13.0-py3-none-any.whl (210 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/210.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m204.8/210.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (5.3.3)\n",
            "Requirement already satisfied: catalogue~=2.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (2.0.10)\n",
            "Collecting cytoolz>=0.10.1 (from textacy)\n",
            "  Downloading cytoolz-0.12.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting floret~=0.10.0 (from textacy)\n",
            "  Downloading floret-0.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jellyfish>=0.8.0 (from textacy)\n",
            "  Downloading jellyfish-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.3.2)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from textacy) (3.2.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.25.2)\n",
            "Collecting pyphen>=0.10.0 (from textacy)\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.2.2)\n",
            "Requirement already satisfied: spacy~=3.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (3.7.4)\n",
            "Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.10/dist-packages (from textacy) (4.66.2)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from cytoolz>=0.10.1->textacy) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (2024.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->textacy) (3.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (2.4.8)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (6.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (4.10.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy~=3.0->textacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy~=3.0->textacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy~=3.0->textacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy~=3.0->textacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy~=3.0->textacy) (2.1.5)\n",
            "Installing collected packages: pyphen, jellyfish, floret, cytoolz, textacy\n",
            "Successfully installed cytoolz-0.12.3 floret-0.10.5 jellyfish-1.0.3 pyphen-0.14.0 textacy-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "import string\n",
        "import os\n",
        "from nltk.parse.corenlp import CoreNLPServer\n",
        "from nltk.parse.corenlp import CoreNLPParser\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tree.tree import Tree\n",
        "import re\n",
        "import textacy\n",
        "from textacy import *\n",
        "from gensim.models import Word2Vec\n",
        "from scipy.spatial import distance\n",
        "import networkx as nx\n"
      ],
      "metadata": {
        "id": "gcd6cp_IfcLs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['CLASSPATH'] = 'stanford-corenlp-4.5.6'  #setting environment variable"
      ],
      "metadata": {
        "id": "_AJi_x9Zivya"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class News_headline_generation:\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "        self.sentences = sent_tokenize(self.df)\n",
        "\n",
        "    def pre_process(self):\n",
        "        sentences_clean = [re.sub(r'[^\\w\\s]', '', sentence.lower()) for sentence in self.sentences]\n",
        "        stop_words = stopwords.words('english')\n",
        "        sentence_tokens = [[words for words in sentence.split(' ') if words not in stop_words] for sentence in sentences_clean]\n",
        "        return sentence_tokens\n",
        "\n",
        "    # def count_paragraphs(self):\n",
        "    #     val=self.df\n",
        "    #     paragraphs = re.split(r\"\\n\\n+\",val)\n",
        "    #     num_paragraphs = len(paragraphs)\n",
        "    #     print(\"num_paragraphs: \",num_paragraphs)\n",
        "\n",
        "    def count_paragraphs(self):\n",
        "        text=self.df\n",
        "        paragraphs = re.split(r'\\n\\s*\\n', text)\n",
        "        return (paragraphs,len(paragraphs))\n",
        "\n",
        "    def word2vec(self):\n",
        "        sentence_tokens = self.pre_process()\n",
        "        w2v = Word2Vec(sentence_tokens, vector_size=1, min_count=1, epochs=1000)\n",
        "        sentence_embeddings = []\n",
        "        max_len = max(len(tokens) for tokens in sentence_tokens)\n",
        "        for words in sentence_tokens:\n",
        "            embedding = [w2v.wv[word] for word in words]\n",
        "            padding_length = max_len - len(embedding)\n",
        "            padded_embedding = np.pad(embedding, [(0, padding_length), (0, 0)], mode='constant')\n",
        "            sentence_embeddings.append(padded_embedding)\n",
        "        return sentence_embeddings\n",
        "\n",
        "    def similarity_matrix(self):\n",
        "        sentence_tokens = self.pre_process()\n",
        "        sentence_embeddings = self.word2vec()\n",
        "        similarity_matrix = np.zeros([len(sentence_tokens), len(sentence_tokens)])\n",
        "        for i, row_embedding in enumerate(sentence_embeddings):\n",
        "            for j, column_embedding in enumerate(sentence_embeddings):\n",
        "                similarity_matrix[i][j] = 1 - distance.cosine(row_embedding.ravel(), column_embedding.ravel())\n",
        "        return similarity_matrix\n",
        "\n",
        "    def num_of_leadingsentences(self):\n",
        "        num_sentences = len(self.sentences)\n",
        "        if num_sentences < 5:\n",
        "            top = 1\n",
        "        elif num_sentences < 10:\n",
        "            top = 2\n",
        "        elif num_sentences < 25:\n",
        "            top = 4\n",
        "        elif num_sentences < 50:\n",
        "            top = 9\n",
        "        elif num_sentences < 100:\n",
        "            top = 18\n",
        "        elif num_sentences < 200:\n",
        "            top = 25\n",
        "        elif num_sentences >= 201:\n",
        "            top = 40\n",
        "        return top\n",
        "\n",
        "    def text_rank(self,num_sentences_to_extract):\n",
        "        li=[]\n",
        "        similarity_matrixs = self.similarity_matrix()\n",
        "        nx_graph = nx.from_numpy_array(similarity_matrixs)\n",
        "        scores = nx.pagerank(nx_graph)\n",
        "        top_sentence = {sentence: scores[index] for index, sentence in enumerate(self.sentences)}\n",
        "        top = dict(sorted(top_sentence.items(), key=lambda x: x[1], reverse=True)[:num_sentences_to_extract])\n",
        "        for sent in self.sentences:\n",
        "            if sent in top.keys():\n",
        "                li.append(sent)\n",
        "        return li\n",
        "\n",
        "    def leading_sentences(self):\n",
        "        article_info = self.count_paragraphs()\n",
        "        leading_sentences=[]\n",
        "        #if there is only one para in article then num_of_leading sentences are selected based on fixed constant\n",
        "        if article_info[1]==1:\n",
        "          num_sentences_to_extract=self.num_of_leadingsentences()\n",
        "          LSG_article = News_headline_generation(str(article_info[0]))\n",
        "          leading_sentences.extend(LSG_article.text_rank(num_sentences_to_extract))\n",
        "          #leading_sentences_corpus.append(leading_sentences)\n",
        "        else:\n",
        "          num_sentences_to_extract=1                   #if there are more than one paras in article\n",
        "          paragraphs = article_info[0]\n",
        "          #print(\"num_paras: \",paragraphs)\n",
        "          #extracting one leading sentence from each paragraph\n",
        "          for para in paragraphs:\n",
        "              LSG = News_headline_generation(para)\n",
        "              output = LSG.text_rank(num_sentences_to_extract)\n",
        "              leading_sentences.extend(output)\n",
        "          #extractig leading sentence from entire article\n",
        "          LSG_article = News_headline_generation(para)\n",
        "          leading_sentences.extend(LSG_article.text_rank(num_sentences_to_extract))\n",
        "\n",
        "        return leading_sentences\n",
        "\n",
        "    def Parsing(self,Sentence, server):\n",
        "        parser = CoreNLPParser(url=server.url)\n",
        "        return next(parser.raw_parse(Sentence))\n",
        "\n",
        "    def find_leftmost_S(self,tree):\n",
        "        if isinstance(tree, str):  # Terminal node\n",
        "            return None\n",
        "        elif tree.label() == 'S':  # Found leftmost S node\n",
        "            return tree\n",
        "        else:\n",
        "            for subtree in tree:\n",
        "                result = self.find_leftmost_S(subtree)\n",
        "                if result is not None:\n",
        "                    return result\n",
        "\n",
        "    def Pruning(self,tree, Label):\n",
        "          if isinstance(tree, str):\n",
        "            return tree\n",
        "          if tree.height() > 0:\n",
        "            filtered_children = [self.Pruning(child, Label) for child in tree if (isinstance(child, str) or child.height() > 0) and (isinstance(child, str) or child.label() != Label)]\n",
        "            return Tree(tree.label(), filtered_children)\n",
        "          else:\n",
        "            return tree\n",
        "\n",
        "    def Extract(self,tree):\n",
        "          words = []\n",
        "          for subtree in tree:\n",
        "              if isinstance(subtree, Tree):\n",
        "                  words.extend(self.Extract(subtree))\n",
        "              else:\n",
        "                  words.append(subtree)\n",
        "          return words\n",
        "\n",
        "    def low_content_words(word_list):\n",
        "          stop_words = set(stopwords.words('english'))\n",
        "          words = [word.lower() for word in word_list]\n",
        "          filtered_li=[]\n",
        "          for word in words:\n",
        "            if word not in stop_words:\n",
        "              filtered_li.append(word)\n",
        "          #print(\"filtered_li: \",filtered_li)\n",
        "          word_freq = Counter(filtered_li)\n",
        "          total_words = len(words)\n",
        "          low_content_threshold = 0.01\n",
        "          low_content_words = {word for word, freq in word_freq.items() if freq >= low_content_threshold * total_words}\n",
        "          return low_content_words\n",
        "\n",
        "    def Matching(TFdict, text):\n",
        "          translator = str.maketrans('', '', string.punctuation)\n",
        "          text = text.translate(translator)\n",
        "          words = word_tokenize(text)\n",
        "\n",
        "          porter_stemmer = PorterStemmer()\n",
        "          stemmed_words = [porter_stemmer.stem(word.lower()) for word in words]\n",
        "\n",
        "          count = 0\n",
        "          for i in stemmed_words:\n",
        "            if i in TFdict.keys():\n",
        "              count += TFdict[i]\n",
        "          return count\n",
        "\n",
        "    def SGRMatching(self, HeadLine, TopPhrases):\n",
        "          l, Flag, itre = len(TopPhrases), 0.0, 0\n",
        "          for Phrase in TopPhrases:\n",
        "            if Phrase in HeadLine:\n",
        "              Flag += (l - TopPhrases.index(Phrase)) / l\n",
        "              itre += 1\n",
        "              #print(f\"{Phrase} : {TopPhrases.index(Phrase)}\")\n",
        "          #print(f\"Cumiliative Value : {Flag / itre}\")\n",
        "          if itre != 0:\n",
        "            return Flag / itre\n",
        "          else:\n",
        "            return -1\n",
        "\n",
        "    def KeyPhraseSGRank(self):\n",
        "          en = textacy.load_spacy_lang(\"en_core_web_sm\")\n",
        "\n",
        "          doc = textacy.make_spacy_doc(self.df, lang=en)\n",
        "\n",
        "          TopPhrases = [kps for kps, weights in textacy.extract.keyterms.sgrank(doc, topn=1.0)]\n",
        "\n",
        "          return TopPhrases\n",
        "\n",
        "    def Generate(self):\n",
        "        CompressedSentences = []\n",
        "        server = CoreNLPServer()\n",
        "        server.start()\n",
        "        leading_sentences_list = self.leading_sentences()\n",
        "        for i in leading_sentences_list:\n",
        "          ParsedSentence = self.Parsing(i, server)\n",
        "          for i in ParsedSentence:\n",
        "            for j in i:\n",
        "              lefts = self.find_leftmost_S(j)\n",
        "              if lefts is not None:\n",
        "                LeftMostS = lefts\n",
        "              else:\n",
        "                LeftMostS = i\n",
        "              break\n",
        "          Labels = ['DT', 'TMP', 'SBAR']\n",
        "          #, 'CC', 'VBZ, 'PRP'' IN',\n",
        "          for i in Labels:\n",
        "            Temp = self.Pruning(LeftMostS, i)\n",
        "            LeftMostS = Temp\n",
        "          word_list = self.Extract(Temp)\n",
        "          #low_content_words_inpara=low_content_words(word_list)\n",
        "          sentence = ' '.join(word_list)\n",
        "          CompressedSentences.append(sentence)\n",
        "        server.stop()\n",
        "\n",
        "        ResultDict = {}\n",
        "        KeyPhrases = self.KeyPhraseSGRank()\n",
        "        for i in CompressedSentences:\n",
        "          #ResultDict[i] = Matching(TFdict, i)\n",
        "          ResultDict[i] = self.SGRMatching(i, KeyPhrases)\n",
        "\n",
        "        #print(ResultDict)\n",
        "\n",
        "        max_key = max(ResultDict, key=lambda k: ResultDict[k])\n",
        "        #print(\"max_key: \",max_key)\n",
        "        return max_key"
      ],
      "metadata": {
        "id": "pwJWk71ahdjp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXAMPLE 1**"
      ],
      "metadata": {
        "id": "04CRVd1w1kzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Article = \"\"\"IDH’s Life and Building Safety (LABS) Initiative is delighted to announce a partnership with Nike, a leading athletic footwear, apparel, and accessories manufacturer. This collaboration is a significant step forward in the ongoing commitment to fostering safer working conditions and sustainable practices within the global supply chain.\n",
        "\n",
        "As part of this collaboration, the LABS Initiative will work closely with factories supplying to Nike to enhance safety standards, ensuring they undergo comprehensive assessments and safety training processes.\n",
        "\n",
        "LABS brings together industry leaders, non-governmental organizations, and stakeholders to collectively address and enhance life and building safety in factories worldwide. Nike’s decision to join the LABS Initiative signifies a shared commitment to elevating safety standards in manufacturing facilities and promoting responsible and sustainable business practices and further strengthens the foundation pillar for the program.\n",
        "\n",
        "The LABS Initiative is excited to embark on this journey with Nike, collectively driving positive change and setting new benchmarks for safety and sustainability in the athletic apparel and footwear industry.\"\"\""
      ],
      "metadata": {
        "id": "P6_Y-WL2yj9v"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NHG = News_headline_generation(Article)\n",
        "output = NHG.Generate()\n",
        "print(\"Output is: \",output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hkQfG76yu0o",
        "outputId": "119c4b7a-3d51-47ee-cd6c-df73537cf763"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output is:  As part of collaboration , LABS Initiative will work closely with factories supplying to Nike to enhance safety standards , ensuring .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXAMPLE 2**"
      ],
      "metadata": {
        "id": "ti-VUw0C1no6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Article = \"\"\"Einar Bessi Gestsson, a natural disaster expert at the Norwegian Meteorological Agency, has told Iceland's public broadcaster RUV that dangerous gases and small explosions could occur if lava makes contact with sea water.\n",
        "\n",
        "Meanwhile, the lava moving west is heading in the direction of the Blue Lagoon and a geothermal power plant, which provides hot water for most of the Reykjanes Peninsula.\n",
        "\n",
        "The Icelandic Met Office said this lava bed was \"significantly wider\" than in February, when an earlier eruption caused lava to flow in a similar direction.\n",
        "\n",
        "Many protective embankments have been built around both, the head of the Reykjavik-based Nordic Volcanological Centre, Rikke Pedersen, told Reuters.\n",
        "\n",
        "There are concerns that fibre optic cables on the road could be damaged - causing disruption to phone and internet services.\"\"\""
      ],
      "metadata": {
        "id": "fnOT2VAW1qFf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NHG = News_headline_generation(Article)\n",
        "output = NHG.Generate()\n",
        "print(\"Output is: \",output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9YYkif_1zUB",
        "outputId": "2ea5d467-5a9f-4443-d4f3-a3e1ee6b0b9c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output is:  Icelandic Met Office said .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXAMPLE 3**"
      ],
      "metadata": {
        "id": "ozzuQbNV2QTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Article = \"\"\"Qatar's foreign ministry spokesman says the passing on Monday of a UN Security Council resolution calling for a ceasefire in Gaza has had no \"immediate impact\" on the ongoing negotiations between Israel and Hamas.\n",
        "\n",
        "These talks, which are aimed at agreeing a temporary truce and the release of hostages still being held by Hamas, are currently taking place in Doha with the input of mediators from Qatar, Egypt and the US.\n",
        "\n",
        "According to the Reuters news agency, sources briefed on the discussions said they were \"moving ahead\" and that while some of the Israeli delegation has left Doha, a smaller group of representatives remains.\n",
        "\n",
        "Israel's Haaretz newspaper also cited an Israeli source as stating that \"the deal has not collapsed\".\n",
        "\n",
        "However, Israeli Prime Minister Benjamin Netanyahu warned in a statement that the UN resolution had \"damaged\" the negotiations and encouraged Hamas to stick to its \"delusional demands\".\n",
        "\n",
        "Hamas says a ceasefire should mean the end of the war and the withdrawal of Israeli forces from Gaza, which Netanyahu rejects.\"\"\"\n",
        "\n",
        "#original headline: UN ceasefire resolution has had no 'immediate impact' on talks - Qatar"
      ],
      "metadata": {
        "id": "Vo2JKLqa2TEE"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NHG = News_headline_generation(Article)\n",
        "output = NHG.Generate()\n",
        "print(\"Output is: \",output)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCERK_632X_B",
        "outputId": "0adc047c-7557-44ee-8934-609710a5427e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output is:  Qatar 's foreign ministry spokesman says .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 4**"
      ],
      "metadata": {
        "id": "OJ8xymvh3OQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Article = \"\"\"As we’ve been reporting, fighting is continuing in Gaza - despite the UN Security Council resolution calling for a ceasefire being passed for the first time on Monday.\n",
        "\n",
        "Ohad Tal is an Israeli MP from the far-right Religious Zionism party, which is part of Prime Minister Benjamin Netanyahu's coalition.\n",
        "\n",
        "Speaking to the BBC World Service’s Newsday programme earlier, he was asked why Israel is continuing its offensive in Gaza in the face of such a resolution.\n",
        "\n",
        "“We have a right to defend ourselves – it’s as simple as that,” Tal replied.\n",
        "\n",
        "He said the world must \"make a choice\" between supporting Israel or Hamas.\"\"\"\n",
        "\n",
        "#original headline: We have a right to defend ourselves – Israeli MP"
      ],
      "metadata": {
        "id": "BXQWSJs025-V"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NHG = News_headline_generation(Article)\n",
        "output = NHG.Generate()\n",
        "print(\"Output is: \",output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc6yhqPi3CKO",
        "outputId": "8fedad44-4a63-4002-9422-525809d352e5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output is:  Speaking to BBC World Service ’s Newsday programme earlier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yPDWHHuX3DyG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}